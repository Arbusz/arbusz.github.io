<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分布式协议 on Arbus&#39;s Little Site</title>
    <link>https://arbusz.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/</link>
    <description>Recent content in 分布式协议 on Arbus&#39;s Little Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Sep 2018 17:08:32 +0800</lastBuildDate>
    
	<atom:link href="https://arbusz.github.io/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Raft协议</title>
      <link>https://arbusz.github.io/2018/09/raft%E5%8D%8F%E8%AE%AE/</link>
      <pubDate>Sun, 02 Sep 2018 17:08:32 +0800</pubDate>
      
      <guid>https://arbusz.github.io/2018/09/raft%E5%8D%8F%E8%AE%AE/</guid>
      <description>分布式存储系统通常采用多个副本进行容错，提高系统的可用性。要实现此目标，就必须要解决分布式存储系统的最核心问题，维护多个副本的一致性。 &amp;gt; 一致性是构建容错性的分布式系统的基础。在一个具有一致性的性质的集群里，同一时刻所有的节点对存储在其中的某个值都有相同的结果，即对其共享的存储保持一致性。集群具有自动恢复的性质，少数节点失效时不会影响整个集群的正常工作。
说白了，一致性就是保证即使在部分副本宕机时，系统仍然能正常对外提供服务。一致性协议通常基于replicated state machines，即所有结点都从同一个state出发，都经过同样的一些操作序列（log），最后到达同样的state。 &amp;gt;系统中每个节点有三个组件 &amp;gt;- 状态机：当我们说一致性的时候，实际就是在说要保证这个状态机的一致性。状态机会从log里面取出所有的命令，然后执行一遍，得到的结果就是我们对外提供的保证了一致性的数据 &amp;gt;- log：保存了所有修改记录 &amp;gt;- 一致性模块：一致性模块算法就是用来保证写入的log的命令的一致性，这也是raft算法核心内容
Raft协议将一致性协议的核心内容分拆成为几个关键阶段，以简化流程，提高协议的可理解性。
1. Leader election（选主） Raft协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate。 &amp;gt;- Leader：所有请求的处理者，Leader副本接受client的更新请求，本地处理后再同步至多个其他副本； &amp;gt;- Follower：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件 &amp;gt;- Candidate：如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束。
时间被分为很多连续的随机长度term，term有唯一的Id。每个id一开始就进行选主。 - 1.Follower将自己维护的current_term_id加1 - 2.将自己的状态转为Candidate - 3.发送RequestVoteRPC消息（附带current_term_id）给其他所有server
此过程会有三种结果： - 自己被选为主。当收到majority的投票后，状态切成了Leader，并定期给其他的所有的server发心跳消息（不带log的AppendEntriesRPC）以告诉对方自己是current_term_id所标识的term的Leader。每个term最多有一个leader，term id作为logical lock，在每个RPC消息中都会带上，用于检测过期的消息。当一个server收到的RPC消息中的rpc_term_id比本地的current_term_id更大时，就更新 current_term_id为rpc_term_id，并且如果当前state为Leader或者Candidate时，将自己的状态切为Follower。如果更小，则拒绝这个消息。 - 别人成了主。当Candidate在等待投票的过程中，收到了大于或者等与本地的current_term_id声明对方是Leader的AppendEnteriesRPC时，则将自己的的state切换为Follower，并更新自己本地的current_term_id。 - 没有选出主。没有Leader被选出，每个Candidate等待投票的额过程就已超时，接着Candidates就会将本地的current_term_id再加一，发起RequestVoteRPC进行新一轮的Leader election。
投票策略： - 每个节点只会给每个term投一票，具体的是否同意和后续的Safety有关。 - 当投票被瓜分后，所有的Candidate同时超时，然后有可能进入新一轮的票数被瓜分。为了避免这个问题，Raft采用一种很简单的方法：每个Candidate的election timeout从150ms-300ms之间随机取，那么第一个超时的Candidate就可以发起新一轮的leader election，带着最大的term_id给其它所有server发送RequestVoteRPC消息，从而自己成为leader，然后给他们发送心跳消息以告诉他们自己是主。
 Raft有2个timeout设置 1）从follow而转换到candidate的timeout： election timeout，设置为：150ms到300ms中的随机数。一个node到达这个timeout之后会发起一个新的选举term（递增的，大的表示新的），向其他节点发起投票请求，包括投给自己的那票，如果获得了大多数选票，那么自己就转换为leader状态 2）node成为leader之后会向其他node发送Append Entries，这个时间为heartbeat timeout 如果lead在实际使用中down掉，剩下的节点会重新开启1）和2）描述的选举流程，保证了高可用性 特殊情况:
如果集群中剩下偶数个node，并且在选举的过程中有2个node获得相等的选票数，那么会开启新的一轮term选举。直到有一个node获得多数选票（随机的election timeout保证可行）  2. Log Replication 当Leader被选出来后，就可以接受客户端发来的请求了，每个请求包含一条需要被replicated state machines执行的命令。leader会把它作为一个log entry append到日志中，然后给其它的server发AppendEntriesRPC请求。当Leader确定一个log entry被safely replicated了（大多数副本已经将该命令写入日志当中），就apply这条log entry到状态机中然后返回结果给客户端。如果某个Follower宕机了或者运行的很慢，或者网络丢包了，则会一直给这个Follower发AppendEntriesRPC直到日志一致。</description>
    </item>
    
  </channel>
</rss>