<!DOCTYPE html>
<html class="nojs" lang="en" dir="ltr">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=2.0, user-scalable=yes">
<title>Raft协议 – Hugo Zen theme</title>
<meta name="created" content="2018-09-02T17:08:32&#43;0800">
<meta name="modified" content="2018-09-02T17:08:32&#43;0800">

<meta name="description" content="分布式存储系统通常采用多个副本进行容错，提高系统的可用性。要实现此目标，就必须要解决分布式存储系统的最核心问题，维护多个副本的一致性。 &amp;gt; 一致性是构建容错性的分布式系统的基础。在一个具有一致性的性质的集群里，同一时刻所有的节点对存储在其中的某个值都有相同的结果，即对其共享的存储保持一致性。集群具有自动恢复的性质，少数节点失效时不会影响整个集群的正常工作。 说白了，一致性就是保证即使在部分副本宕机时，系统仍然能正常对外提供服务。一致性协议通常基于replicated state machines，即所有结点都从同一个state出发，都经过同样的一些操作序列（log），最后到达同样的state。 &amp;gt;系统中每个节点有三个组件 &amp;gt;- 状态机：当我们说一致性的时候，实际就是在说要保证这个状态机的一致性。状态机会从log里面取出所有的命令，然后执行一遍，得到的结果就是我们对外提供的保证了一致性的数据 &amp;gt;- log：保存了所有修改记录 &amp;gt;- 一致性模块：一致性模块算法就是用来保证写入的log的命令的一致性，这也是raft算法核心内容 Raft协议将一致性协议的核心内容分拆成为几个关键阶段，以简化流程，提高协议的可理解性。 1. Leader election（选主） Raft协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate。 &amp;gt;- Leader：所有请求的处理者，Leader副本接受client的更新请求，本地处理后再同步至多个其他副本； &amp;gt;- Follower：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件 &amp;gt;- Candidate：如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束。 时间被分为很多连续的随机长度term，term有唯一的Id。每个id一开始就进行选主。 - 1.Follower将自己维护的current_term_id加1 - 2.将自己的状态转为Candidate - 3.发送RequestVoteRPC消息（附带current_term_id）给其他所有server 此过程会有三种结果： - 自己被选为主。当收到majority的投票后，状态切成了Leader，并定期给其他的所有的server发心跳消息（不带log的AppendEntriesRPC）以告诉对方自己是current_term_id所标识的term的Leader。每个term最多有一个leader，term id作为logical lock，在每个RPC消息中都会带上，用于检测过期的消息。当一个server收到的RPC消息中的rpc_term_id比本地的current_term_id更大时，就更新 current_term_id为rpc_term_id，并且如果当前state为Leader或者Candidate时，将自己的状态切为Follower。如果更小，则拒绝这个消息。 - 别人成了主。当Candidate在等待投票的过程中，收到了大于或者等与本地的current_term_id声明对方是Leader的AppendEnteriesRPC时，则将自己的的state切换为Follower，并更新自己本地的current_term_id。 - 没有选出主。没有Leader被选出，每个Candidate等待投票的额过程就已超时，接着Candidates就会将本地的current_term_id再加一，发起RequestVoteRPC进行新一轮的Leader election。 投票策略： - 每个节点只会给每个term投一票，具体的是否同意和后续的Safety有关。 - 当投票被瓜分后，所有的Candidate同时超时，然后有可能进入新一轮的票数被瓜分。为了避免这个问题，Raft采用一种很简单的方法：每个Candidate的election timeout从150ms-300ms之间随机取，那么第一个超时的Candidate就可以发起新一轮的leader election，带着最大的term_id给其它所有server发送RequestVoteRPC消息，从而自己成为leader，然后给他们发送心跳消息以告诉他们自己是主。  Raft有2个timeout设置 1）从follow而转换到candidate的timeout： election timeout，设置为：150ms到300ms中的随机数。一个node到达这个timeout之后会发起一个新的选举term（递增的，大的表示新的），向其他节点发起投票请求，包括投给自己的那票，如果获得了大多数选票，那么自己就转换为leader状态 2）node成为leader之后会向其他node发送Append Entries，这个时间为heartbeat timeout 如果lead在实际使用中down掉，剩下的节点会重新开启1）和2）描述的选举流程，保证了高可用性 特殊情况: 如果集群中剩下偶数个node，并且在选举的过程中有2个node获得相等的选票数，那么会开启新的一轮term选举。直到有一个node获得多数选票（随机的election timeout保证可行）  2. Log Replication 当Leader被选出来后，就可以接受客户端发来的请求了，每个请求包含一条需要被replicated state machines执行的命令。leader会把它作为一个log entry append到日志中，然后给其它的server发AppendEntriesRPC请求。当Leader确定一个log entry被safely replicated了（大多数副本已经将该命令写入日志当中），就apply这条log entry到状态机中然后返回结果给客户端。如果某个Follower宕机了或者运行的很慢，或者网络丢包了，则会一直给这个Follower发AppendEntriesRPC直到日志一致。">
<meta property="og:site_name" content="Hugo Zen theme">
<meta property="og:title" content="Raft协议">
<meta property="og:url" content="https://arbusz.github.io/post/raft%E5%8D%8F%E8%AE%AE/">
<meta name="og:image" content="https://arbusz.github.io/xdeb-org.jpg">
<meta name="generator" content="Hugo 0.40.1" />
<meta name="contact" content="info@example.org">
<link rel="canonical" href="https://arbusz.github.io/post/raft%E5%8D%8F%E8%AE%AE/">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">
<link rel="stylesheet" href="/css/styles.css">
<link rel="stylesheet" href="/css/print.css" media="print">
<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Raft协议",
    "datePublished": "2018-09-02T17:08:32+08:00",
    "dateModified": "2018-09-02T17:08:32+08:00",
    "url" : "https://arbusz.github.io/post/raft%E5%8D%8F%E8%AE%AE/",
    "description": "分布式存储系统通常采用多个副本进行容错，提高系统的可用性。要实现此目标，就必须要解决分布式存储系统的最核心问题，维护多个副本的一致性。 \u0026gt; 一致性是构建容错性的分布式系统的基础。在一个具有一致性的性质的集群里，同一时刻所有的节点对存储在其中的某个值都有相同的结果，即对其共享的存储保持一致性。集群具有自动恢复的性质，少数节点失效时不会影响整个集群的正常工作。 说白了，一致性就是保证即使在部分副本宕机时，系统仍然能正常对外提供服务。一致性协议通常基于replicated state machines，即所有结点都从同一个state出发，都经过同样的一些操作序列（log），最后到达同样的state。 \u0026gt;系统中每个节点有三个组件 \u0026gt;- 状态机：当我们说一致性的时候，实际就是在说要保证这个状态机的一致性。状态机会从log里面取出所有的命令，然后执行一遍，得到的结果就是我们对外提供的保证了一致性的数据 \u0026gt;- log：保存了所有修改记录 \u0026gt;- 一致性模块：一致性模块算法就是用来保证写入的log的命令的一致性，这也是raft算法核心内容 Raft协议将一致性协议的核心内容分拆成为几个关键阶段，以简化流程，提高协议的可理解性。 1. Leader election（选主） Raft协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate。 \u0026gt;- Leader：所有请求的处理者，Leader副本接受client的更新请求，本地处理后再同步至多个其他副本； \u0026gt;- Follower：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件 \u0026gt;- Candidate：如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束。 时间被分为很多连续的随机长度term，term有唯一的Id。每个id一开始就进行选主。 - 1.Follower将自己维护的current_term_id加1 - 2.将自己的状态转为Candidate - 3.发送RequestVoteRPC消息（附带current_term_id）给其他所有server 此过程会有三种结果： - 自己被选为主。当收到majority的投票后，状态切成了Leader，并定期给其他的所有的server发心跳消息（不带log的AppendEntriesRPC）以告诉对方自己是current_term_id所标识的term的Leader。每个term最多有一个leader，term id作为logical lock，在每个RPC消息中都会带上，用于检测过期的消息。当一个server收到的RPC消息中的rpc_term_id比本地的current_term_id更大时，就更新 current_term_id为rpc_term_id，并且如果当前state为Leader或者Candidate时，将自己的状态切为Follower。如果更小，则拒绝这个消息。 - 别人成了主。当Candidate在等待投票的过程中，收到了大于或者等与本地的current_term_id声明对方是Leader的AppendEnteriesRPC时，则将自己的的state切换为Follower，并更新自己本地的current_term_id。 - 没有选出主。没有Leader被选出，每个Candidate等待投票的额过程就已超时，接着Candidates就会将本地的current_term_id再加一，发起RequestVoteRPC进行新一轮的Leader election。 投票策略： - 每个节点只会给每个term投一票，具体的是否同意和后续的Safety有关。 - 当投票被瓜分后，所有的Candidate同时超时，然后有可能进入新一轮的票数被瓜分。为了避免这个问题，Raft采用一种很简单的方法：每个Candidate的election timeout从150ms-300ms之间随机取，那么第一个超时的Candidate就可以发起新一轮的leader election，带着最大的term_id给其它所有server发送RequestVoteRPC消息，从而自己成为leader，然后给他们发送心跳消息以告诉他们自己是主。  Raft有2个timeout设置 1）从follow而转换到candidate的timeout： election timeout，设置为：150ms到300ms中的随机数。一个node到达这个timeout之后会发起一个新的选举term（递增的，大的表示新的），向其他节点发起投票请求，包括投给自己的那票，如果获得了大多数选票，那么自己就转换为leader状态 2）node成为leader之后会向其他node发送Append Entries，这个时间为heartbeat timeout 如果lead在实际使用中down掉，剩下的节点会重新开启1）和2）描述的选举流程，保证了高可用性 特殊情况: 如果集群中剩下偶数个node，并且在选举的过程中有2个node获得相等的选票数，那么会开启新的一轮term选举。直到有一个node获得多数选票（随机的election timeout保证可行）  2. Log Replication 当Leader被选出来后，就可以接受客户端发来的请求了，每个请求包含一条需要被replicated state machines执行的命令。leader会把它作为一个log entry append到日志中，然后给其它的server发AppendEntriesRPC请求。当Leader确定一个log entry被safely replicated了（大多数副本已经将该命令写入日志当中），就apply这条log entry到状态机中然后返回结果给客户端。如果某个Follower宕机了或者运行的很慢，或者网络丢包了，则会一直给这个Follower发AppendEntriesRPC直到日志一致。",
    "image" : "https://arbusz.github.io/xdeb-org.jpg",
    "keywords": ["分布式"],
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://arbusz.github.io/"
    },
    "publisher": {
      "@type": "Organization",
      "name": "Hugo Zen theme",
      "url": "https://arbusz.github.io/",
    }
  }
</script>

</head>

<body>
<div class="page layout__page layout__sidebar-second">
<header class="header layout__header" role="banner">

<h1 class="header__site-name">
<a href="/" title="Home" class="header__site-link" rel="home"><span>Hugo Zen theme</span></a>
</h1>
<div class="region header__region">


<form class="search-form" method="get" action="https://duckduckgo.com/">
<input type="search" name="q" class="search-text" placeholder="Search" title="Enter the terms you wish to search for." size="20" maxlength="128">
<input type="hidden" name="sites" value="arbusz.github.io">
<input type="submit" value="DuckDuckGo Search" class="hidden">
</form>

</div>
</header>




<nav class="main-menu layout__navigation" role="navigation">
<h2 class="visually-hidden">Main menu</h2>
<ul class="navbar">
<li><a class="" href="/">Home</a></li>
<li><a class="active" href="/post/">Post</a></li>
</ul>
</nav>


<main class="main layout__main" role="main">
<h1 class="title title-submitted">Raft协议</h1>
<article class="section-post single-view">
<header>
<p class="submitted">
<time class="modified-date" datetime="2018-09-02T17:08:32&#43;08:00">2 September, 2018</time>
</p>
<div class="tags">
Tags:
<ul>
<li><a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F">分布式</a></li>
</ul>
</div>
</header>
<div class="content">
<p>
分布式存储系统通常采用多个副本进行容错，提高系统的可用性。要实现此目标，就必须要解决分布式存储系统的最核心问题，维护多个副本的一致性。
&gt; 一致性是构建容错性的分布式系统的基础。在一个具有一致性的性质的集群里，同一时刻所有的节点对存储在其中的某个值都有相同的结果，即对其共享的存储保持一致性。集群具有自动恢复的性质，少数节点失效时不会影响整个集群的正常工作。</p>

<p>说白了，一致性就是保证即使在部分副本宕机时，系统仍然能正常对外提供服务。一致性协议通常基于replicated state machines，即所有结点都从同一个state出发，都经过同样的一些操作序列（log），最后到达同样的state。
&gt;系统中每个节点有三个组件
&gt;- 状态机：当我们说一致性的时候，实际就是在说要保证这个状态机的一致性。状态机会从log里面取出所有的命令，然后执行一遍，得到的结果就是我们对外提供的保证了一致性的数据
&gt;- log：保存了所有修改记录
&gt;- 一致性模块：一致性模块算法就是用来保证写入的log的命令的一致性，这也是raft算法核心内容</p>

<p>Raft协议将一致性协议的核心内容分拆成为几个关键阶段，以简化流程，提高协议的可理解性。</p>

<hr />

<h2 id="1-leader-election-选主">1. Leader election（选主）</h2>

<p>Raft协议的每个副本都会处于三种状态之一：Leader、Follower、Candidate。
&gt;- Leader：所有请求的处理者，Leader副本接受client的更新请求，本地处理后再同步至多个其他副本；
&gt;- Follower：请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件
&gt;- Candidate：如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束。</p>

<p>时间被分为很多连续的随机长度term，term有唯一的Id。每个id一开始就进行选主。
- 1.Follower将自己维护的current_term_id加1
- 2.将自己的状态转为Candidate
- 3.发送RequestVoteRPC消息（附带current_term_id）给其他所有server</p>

<p>此过程会有三种结果：
- 自己被选为主。当收到majority的投票后，状态切成了Leader，并定期给其他的所有的server发心跳消息（不带log的AppendEntriesRPC）以告诉对方自己是current_term_id所标识的term的Leader。每个term最多有一个leader，term id作为logical lock，在每个RPC消息中都会带上，用于检测过期的消息。当一个server收到的RPC消息中的rpc_term_id比本地的current_term_id更大时，就更新
current_term_id为rpc_term_id，并且如果当前state为Leader或者Candidate时，将自己的状态切为Follower。如果更小，则拒绝这个消息。
- 别人成了主。当Candidate在等待投票的过程中，收到了大于或者等与本地的current_term_id声明对方是Leader的AppendEnteriesRPC时，则将自己的的state切换为Follower，并更新自己本地的current_term_id。
- 没有选出主。没有Leader被选出，每个Candidate等待投票的额过程就已超时，接着Candidates就会将本地的current_term_id再加一，发起RequestVoteRPC进行新一轮的Leader election。</p>

<p>投票策略：
- 每个节点只会给每个term投一票，具体的是否同意和后续的Safety有关。
- 当投票被瓜分后，所有的Candidate同时超时，然后有可能进入新一轮的票数被瓜分。为了避免这个问题，Raft采用一种很简单的方法：每个Candidate的election timeout从150ms-300ms之间随机取，那么第一个超时的Candidate就可以发起新一轮的leader election，带着最大的term_id给其它所有server发送RequestVoteRPC消息，从而自己成为leader，然后给他们发送心跳消息以告诉他们自己是主。</p>

<blockquote>
<p>Raft有2个timeout设置
1）从follow而转换到candidate的timeout： election timeout，设置为：150ms到300ms中的随机数。一个node到达这个timeout之后会发起一个新的选举term（递增的，大的表示新的），向其他节点发起投票请求，包括投给自己的那票，如果获得了大多数选票，那么自己就转换为leader状态
2）node成为leader之后会向其他node发送Append Entries，这个时间为heartbeat timeout
如果lead在实际使用中down掉，剩下的节点会重新开启1）和2）描述的选举流程，保证了高可用性
<em>特殊情况:</em></p>

<h2 id="如果集群中剩下偶数个node-并且在选举的过程中有2个node获得相等的选票数-那么会开启新的一轮term选举-直到有一个node获得多数选票-随机的election-timeout保证可行">如果集群中剩下偶数个node，并且在选举的过程中有2个node获得相等的选票数，那么会开启新的一轮term选举。直到有一个node获得多数选票（随机的election timeout保证可行）</h2>
</blockquote>

<h2 id="2-log-replication">2. Log Replication</h2>

<p>当Leader被选出来后，就可以接受客户端发来的请求了，每个请求包含一条需要被replicated state machines执行的命令。leader会把它作为一个log entry append到日志中，然后给其它的server发AppendEntriesRPC请求。当Leader确定一个log entry被safely replicated了（大多数副本已经将该命令写入日志当中），就apply这条log entry到状态机中然后返回结果给客户端。如果某个Follower宕机了或者运行的很慢，或者网络丢包了，则会一直给这个Follower发AppendEntriesRPC直到日志一致。</p>

<p>当一条日志是commited时，Leader才可以将它应用到状态机中。Raft保证一条commited的log entry已经持久化了并且会被所有的节点执行。</p>

<h4 id="如果leader和其他fellower的日志不同怎么办">如果Leader和其他Fellower的日志不同怎么办</h4>

<p>我们需要一个机制来保证日志的一致性</p>

<p>如图中例子，最上面这个是新Leader，a~f是Follower，每个格子代表一条log entry，格子内的数字代表这个log entry是在哪个term上产生的。</p>

<p>新Leader产生后，就以Leader上的log为准。其它的follower要么少了数据比如b，要么多了数据，比如d，要么既少了又多了数据，比如f。</p>

<p>因此，需要有一种机制来让leader和follower对log达成一致，leader会为每个follower维护一个nextIndex，表示leader给各个follower发送的下一条log entry在log中的index，初始化为leader的最后一条log entry的下一个位置。leader给follower发送AppendEntriesRPC消息，带着(term_id, (nextIndex-1))， term_id即(nextIndex-1)这个槽位的log entry的term_id，follower接收到AppendEntriesRPC后，会从自己的log中找是不是存在这样的log entry，如果不存在，就给leader回复拒绝消息，然后leader则将nextIndex减1，再重复，知道AppendEntriesRPC消息被接收。</p>

<p>以leader和b为例：</p>

<p>初始化，nextIndex为11，leader给b发送AppendEntriesRPC(6,10)，b在自己log的10号槽位中没有找到term_id为6的log entry。则给leader回应一个拒绝消息。接着，leader将nextIndex减一，变成10，然后给b发送AppendEntriesRPC(6, 9)，b在自己log的9号槽位中同样没有找到term_id为6的log entry。循环下去，直到leader发送了AppendEntriesRPC(4,4)，b在自己log的槽位4中找到了term_id为4的log entry。接收了消息。随后，leader就可以从槽位5开始给b推送日志了。</p>

<p><img src="http://upload-images.jianshu.io/upload_images/4593875-a9ace666b3f34c80.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="example" /></p>

<h3 id="safety">Safety</h3>

<h4 id="哪些follower有资格成为leader">哪些follower有资格成为leader?</h4>

<blockquote>
<p>Raft保证被选为新leader的节点拥有所有已提交的log entry，这与ViewStamped Replication不同，后者不需要这个保证，而是通过其他机制从follower拉取自己没有的提交的日志记录</p>
</blockquote>

<p>这个保证是在RequestVoteRPC阶段做的，candidate在发送RequestVoteRPC时，会带上自己的最后一条日志记录的term_id和index，其他节点收到消息时，如果发现自己的日志比RPC请求中携带的更新，拒绝投票。日志比较的原则是，如果本地的最后一条log entry的term id更大，则更新，如果term id一样大，则日志更多的更大(index更大)。</p>

<h4 id="哪些日志记录被认为是commited">哪些日志记录被认为是commited?</h4>

<ol>
<li>Leader正在replicate当前term（即term 2）的日志记录给其它Follower，一旦Leader确认了这条log entry被majority写盘了，这条log entry就被认为是committed。如图中(a)，S1作为当前term即term2的leader，log index为2的日志被majority写盘了，这条log entry被认为是commited。</li>
<li>Leader正在replicate更早的term的log entry给其它Follower。图(b)的状态是这么来的。</li>
</ol>

<p><img src="http://upload-images.jianshu.io/upload_images/4593875-4be0b2c6ee38dcf4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="example2" />
这里面隐含一个很严重的问题：一应用到状态机的日志被截断：
1. 在阶段a，term为2，S1是Leader，且S1写入日志（term, index）为(2, 2)，并且日志被同步写入了S2。
2. 在阶段b，S1离线，触发一次新的选主，此时S5被选为新的Leader，此时系统term为3，且写入了日志（term, index）为（3， 2）。
3. S5尚未将日志推送到Followers变离线了，进而触发了一次新的选主，而之前离线的S1经过重新上线后被选中变成Leader，此时系统term为4，此时S1会将自己的日志同步到Followers，按照上图就是将日志（2， 2）同步到了S3，而此时由于该日志已经被同步到了多数节点（S1, S2, S3），因此，此时日志（2，2）可以被commit了（即更新到状态机）。
4. 在阶段d，S1又很不幸地下线了，系统触发一次选主，而S5有可能被选为新的Leader（这是因为S5可以满足作为主的一切条件：1. term = 3 &gt; 2, 2. 最新的日志index为2，比大多数节点（如S2/S3/S4的日志都新），然后S5会将自己的日志更新到Followers，于是S2、S3中已经被提交的日志（2，2）被截断了。</p>

<p>为了避免这种错误，对协议作出修改
&gt;只允许主节点提交包含当前term的日志</p>

<p>针对上述情况就是：即使日志（2，2）已经被大多数节点（S1、S2、S3）确认了，但是它不能被Commit，因为它是来自之前term(2)的日志，直到S1在当前term（4）产生的日志（4， 3）被大多数Follower确认，S1方可Commit（4，3）这条日志，当然，根据Raft定义，（4，3）之前的所有日志也会被Commit。此时即使S1再下线，重新选主时S5不可能成为Leader，因为它没有包含大多数节点已经拥有的日志（4，3）。</p>

<h4 id="节点之前的网络状况十分不好-有多个leader怎样处理">节点之前的网络状况十分不好，有多个leader怎样处理？</h4>

<p>节点之前的网络状况十分不好，此时会有多个leader，其term也是不同的。
由于commit的修改需要多数通过，那么只有具有最多node的一个集群会commit修改成功。
当网络状况恢复，整个集群的节点会向多数节点的集群同步。这样整个集群中的数据会继续保持一致</p>

<p><strong>综述一下Log Replication</strong>
client给leader发送数据修改请求
leader通过Append Entries在心跳的过程中将修改内容下发到follower nodes
在多数follower 接收了修改内容返回后，leader向client确认
leader向follower发送心跳，具体执行修改操作，此后数据在集群中保持一致</p>

<hr />

<h2 id="3-log-compaction">3. Log Compaction</h2>

<p>在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响availability。Raft采用对整个系统进行snapshot来处理，snapshot之前的日志都可以丢弃。Snapshot技术在Chubby和ZooKeeper系统中都有采用。
&gt;Raft用的方案是：每个副本独立的对自己的系统状态进行Snapshot，并且只能对已经提交的日志记录（已经应用到状态机）进行snapshot。</p>

<p>Snapshot中包含以下内容：
- 日志元数据，最后一条commited log entry的 (log index, last_included_term)。这两个值在Snapshot之后的第一条log entry的AppendEntriesRPC的consistency check的时候会被用上。一旦这个server做完了snapshot，就可以把这条记录的最后一条log index及其之前的所有的log entry都删掉。</p>

<h2 id="系统状态机-存储系统当前状态">- 系统状态机：存储系统当前状态</h2>

<h2 id="4-membership-changes-待补充">4. Membership Changes（待补充）</h2>

</div>
</article>
</main>



<aside class="sidebar layout__second-sidebar" role="complementary">
<section>
<h4 class="menu"><a class="" href="/post/">Post</a></h4>
<ul class="menu">
<li><a class="" href="/post/%E9%85%8D%E7%BD%AEvim%E7%94%A8%E4%BA%8Ego%E5%BC%80%E5%8F%91/">配置vim用于go开发</a></li>
<li><a class="active" href="/post/raft%E5%8D%8F%E8%AE%AE/">Raft协议</a></li>
<li><a class="" href="/post/%E4%B8%A4%E7%A7%8D%E5%B9%B6%E5%8F%91%E6%A8%A1%E5%9E%8B/">两种并发模型</a></li>
<li><a class="" href="/post/%E4%B8%80%E4%B8%AA%E9%81%8D%E5%8E%86%E7%9B%AE%E5%BD%95%E7%9A%84%E5%B0%8F%E7%A8%8B%E5%BA%8F/">一个遍历目录的小程序</a></li>
<li><a class="" href="/post/%E5%8C%85%E5%90%ABnil%E6%8C%87%E9%92%88%E7%9A%84%E6%8E%A5%E5%8F%A3%E5%92%8Cnil%E6%8E%A5%E5%8F%A3/">包含nil指针的接口和nil接口</a></li>
<li><a class="" href="/post/go%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AE%B0%E5%87%BD%E6%95%B0/">Go语言笔记－函数</a></li>
<li><a class="" href="/post/go%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AE%B0%E8%A1%A8%E8%BE%BE%E5%BC%8F/">Go语言笔记－表达式</a></li>
<li><a class="" href="/post/go%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AE%B0%E7%B1%BB%E5%9E%8B/">Go语言笔记－类型</a></li>
<li><a class="" href="/post/%E5%8D%8F%E7%A8%8Bcoroutines/">协程Coroutines</a></li>
</ul>
</section>
</aside>


<footer class="footer layout__footer" role="contentinfo">
<p><!--Creative Commons License-->This site is licensed under a <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.<!--/Creative Commons License--></p>
<p>Powered by <a href="https://gohugo.io/">Hugo</a> and the <a href="https://github.com/frjo/hugo-theme-zen">Zen theme</a>.</p>

</footer>

</div>
<div class="mobile-nav-wrapper hidden">
  <div class="mobile-nav-cover"></div>
  <a href="#navigation" class="mobile-nav-toggle" aria-haspopup="true" role="button">Menu</a>
  <div class="mobile-nav-sheet"></div>
</div>
<script src="/js/jquery.slim.min.js"></script>
<script src="/js/script.js"></script>
</body>
</html>
